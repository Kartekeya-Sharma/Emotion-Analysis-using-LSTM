{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26328673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "527f6f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48998 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "data = pd.read_csv('tweet_emotions.csv')\n",
    "\n",
    "# Define your hyperparameters\n",
    "MAX_NB_WORDS = 5000  # Maximum number of words to tokenize\n",
    "MAX_SEQUENCE_LENGTH = 100  # Maximum length of each sequence\n",
    "EMBEDDING_DIM = 100  # Dimension of the word embeddings\n",
    "VALIDATION_SPLIT = 0.2  # Percentage of data for validation\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(data['content'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21654239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (40000, 100)\n",
      "Shape of label tensor: (40000, 13)\n"
     ]
    }
   ],
   "source": [
    "# Convert text to sequences\n",
    "X = tokenizer.texts_to_sequences(data['content'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "\n",
    "# Convert sentiment labels to one-hot encoding\n",
    "Y = pd.get_dummies(data['sentiment']).values\n",
    "print('Shape of label tensor:', Y.shape)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=VALIDATION_SPLIT, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2a68185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM))  # Remove input_length argument\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d500062c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 - 28s - 57ms/step - accuracy: 0.2797 - loss: 2.0722 - val_accuracy: 0.3375 - val_loss: 1.9491\n",
      "Epoch 2/5\n",
      "500/500 - 25s - 51ms/step - accuracy: 0.3625 - loss: 1.8802 - val_accuracy: 0.3479 - val_loss: 1.9207\n",
      "Epoch 3/5\n",
      "500/500 - 25s - 49ms/step - accuracy: 0.4021 - loss: 1.7871 - val_accuracy: 0.3549 - val_loss: 1.9034\n",
      "Epoch 4/5\n",
      "500/500 - 24s - 48ms/step - accuracy: 0.4307 - loss: 1.7140 - val_accuracy: 0.3456 - val_loss: 1.9207\n",
      "Epoch 5/5\n",
      "500/500 - 24s - 48ms/step - accuracy: 0.4504 - loss: 1.6481 - val_accuracy: 0.3406 - val_loss: 1.9507\n",
      "Validation Accuracy: 0.34062498807907104\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, Y_val), verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, Y_val, verbose=0)\n",
    "print('Validation Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "684fa21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique types of sentiments\n",
    "unique_sentiments = data['sentiment'].unique()\n",
    "\n",
    "# Create index_to_label dictionary\n",
    "index_to_label = {index: sentiment for index, sentiment in enumerate(unique_sentiments)}\n",
    "\n",
    "# Define a function to preprocess and classify a sample sentence\n",
    "def classify_sentiment(sample_sentence):\n",
    "    sequence = tokenizer.texts_to_sequences([sample_sentence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    predicted_probabilities = model.predict(padded_sequence)\n",
    "    predicted_label_index = np.argmax(predicted_probabilities)\n",
    "    predicted_sentiment = index_to_label[predicted_label_index]\n",
    "    return predicted_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b37d5d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Predicted Sentiment: love\n"
     ]
    }
   ],
   "source": [
    "# Test the model with a sample sentence\n",
    "sample_sentence = \"That country sucks!\"\n",
    "\n",
    "predicted_sentiment = classify_sentiment(sample_sentence)\n",
    "print(\"Predicted Sentiment:\", predicted_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de9b0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
